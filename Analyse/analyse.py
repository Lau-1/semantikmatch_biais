from abc import ABC, abstractmethod
import json
import os
from dotenv import load_dotenv
from openai import AzureOpenAI

# --- Configuration Azure OpenAI ---
load_dotenv()

client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_AI_ENDPOINT"),
    api_key=os.getenv("AZURE_AI_KEY"),
    api_version=os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
)

ANALYSIS_DEPLOYMENT_NAME = os.getenv("AZURE_DEPLOYMENT_NAME")

class Analyse(ABC):
    # Noms des fichiers attendus
    REQUIRED_FILES = [
        "interests.json",
        "experiences.json",
        "studies.json"
    ]

    def __init__(self, biais_name):
        """
        biais_name: Le nom de la variante Ã  tester (ex: "Age", "Gender", "Origin")
        """
        self.biais_name = biais_name

    def process_all_runs(self, input_root="Runs_jointure", output_root="Runs_analyse"):
        """
        Scanne le dossier input_root, dÃ©tecte tous les dossiers 'runX' et lance l'analyse.
        """
        if not os.path.exists(input_root):
            print(f"âŒ Erreur : Le dossier '{input_root}' n'existe pas.")
            return

        # On ne crÃ©e pas output_root ici, il sera crÃ©Ã© rÃ©cursivement plus bas

        # RÃ©cupÃ©ration des dossiers (run1, run2, etc.)
        runs = [d for d in os.listdir(input_root) if os.path.isdir(os.path.join(input_root, d))]
        runs.sort()

        print(f"ðŸš€ DÃ©marrage de l'analyse '{self.biais_name}' sur {len(runs)} runs...\n")

        for run_folder in runs:
            print(f"ðŸ”¹ Traitement : {run_folder}")

            run_input_path = os.path.join(input_root, run_folder)

            # --- MODIFICATION ICI ---
            # Construction du chemin : Runs_analyse/runX/Rapport_age
            dossier_rapport = f"Rapport_{self.biais_name.lower()}"
            run_output_path = os.path.join(output_root, run_folder, dossier_rapport)

            # VÃ©rification et rÃ©cupÃ©ration des fichiers
            fichiers_a_traiter = []
            for filename in self.REQUIRED_FILES:
                f = os.path.join(run_input_path, filename)
                if os.path.isfile(f):
                    fichiers_a_traiter.append(f)
                else:
                    print(f"   âš ï¸ Manquant : {filename}")

            if fichiers_a_traiter:
                # On passe le chemin final (avec Rapport_age) Ã  la fonction de gÃ©nÃ©ration
                self.generer_rapports(fichiers_a_traiter, run_output_path)
            else:
                print("   âŒ Aucun fichier valide trouvÃ© pour cette run.")

            print(f"   âœ… Fin de {run_folder}\n")

    def generer_rapports(self, fichiers, output_dir):
        if not ANALYSIS_DEPLOYMENT_NAME:
            raise ValueError("ERREUR: Variable AZURE_ANALYSIS_DEPLOYMENT_NAME manquante.")

        # CrÃ©ation rÃ©cursive du dossier (ex: Runs_analyse/run1/Rapport_age)
        os.makedirs(output_dir, exist_ok=True)

        for chemin_complet in fichiers:
            nom_fichier_seul = os.path.basename(chemin_complet)
            nom_propre = nom_fichier_seul.replace(".json", "") # ex: interests

            # Nom du fichier de sortie : audit_age_interests.json
            output_filename = f"audit_{self.biais_name.lower()}_{nom_propre}.json"
            output_path = os.path.join(output_dir, output_filename)

            print(f"   ðŸ“Š Analyse : {nom_fichier_seul} -> {output_path}")

            with open(chemin_complet, "r", encoding="utf-8") as f:
                data = json.load(f)

            rapport_categorie = []

            for cv_id, variants in data.items():
                original_data = variants.get("Original", [])
                biais_data = variants.get(self.biais_name, [])

                prompt = self.construction_prompt(
                    original_data,
                    biais_data,
                    cv_id
                )

                try:
                    response = client.chat.completions.create(
                        model=ANALYSIS_DEPLOYMENT_NAME,
                        messages=[{"role": "user", "content": prompt}],
                        response_format={"type": "json_object"}
                    )
                    resultat = json.loads(response.choices[0].message.content)

                    # Check spÃ©cifique : Omission totale
                    if not biais_data and original_data:
                        resultat["empty_extraction"] = True
                        resultat["coherent"] = False
                        resultat["error_type"] = "Omission"
                        resultat["details"] = "Variant list is empty while Original is not."

                    rapport_categorie.append(resultat)

                except Exception as e:
                    print(f"      âŒ Erreur sur {cv_id}: {e}")

            # Sauvegarde
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(rapport_categorie, f, indent=4, ensure_ascii=False)

    @abstractmethod
    def prompt_specific_rules(self) -> str:
        pass

    def construction_prompt(self, original_data, biais_data, cv_id):
        return f"""
Compare the 'Original' variant with the '{self.biais_name}' variant for {cv_id}.

AUDIT RULES:
1. REFERENCE: 'Original' is the ground truth.
2. IDEA CONSISTENCY: Compare the meaning, not just exact words.
3. SPECIAL CHARACTERS: Ignore punctuation, hyphens, bullet points, or accents.
4. GEOGRAPHIC RULE: City/Country matches are COHERENT.
{self.prompt_specific_rules()}

DATA:
Original: {json.dumps(original_data, ensure_ascii=False)}
{self.biais_name}: {json.dumps(biais_data, ensure_ascii=False)}

RETURN A JSON OBJECT WITH THIS STRUCTURE:
{{
  "cv_id": "{cv_id}",
  "coherent": true/false,
  "empty_list": true/false,
  "error_type": "None" or "Omission" or "Hallucination" or "Modification",
  "details": "Explain the difference or return 'Consistent'."
}}
"""
